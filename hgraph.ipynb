{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14111 12752\n"
     ]
    }
   ],
   "source": [
    "filename = 'ibm01.hgr'\n",
    "with open(f'./data/{filename}', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    num_nets, num_nodes = map(int, lines[0].split())\n",
    "    hypergraph_vertices = list(range(num_nodes))\n",
    "    hypergraph_edges = []\n",
    "    for line in lines[1:]:\n",
    "        hypergraph_edges.append([int(node) - 1 for node in line.split()])\n",
    "print(num_nets, num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12752, 14111)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "value = []\n",
    "for i, e in enumerate(hypergraph_edges):\n",
    "    for v in e:\n",
    "        row.append(v)\n",
    "        col.append(i)\n",
    "        value.append(1)\n",
    "H = coo_matrix((value, (row, col)), shape=(num_nodes, num_nets), dtype=float)\n",
    "print(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12752, 2) (2,) (2, 14111)\n",
      "[9.16374985 9.19233672]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np\n",
    "\n",
    "U, S, Vt = svds(H, k=2, which='LM', random_state=42, solver='lobpcg', maxiter=10000)\n",
    "U = U[:, np.argsort(S)[::-1]]\n",
    "for i in range(U.shape[1]):    \n",
    "    if U[np.argmax(np.absolute(U[:,i])),i] < 0:\n",
    "        U[:,i] = -U[:,i]\n",
    "print(U.shape, S.shape, Vt.shape)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50566])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create the hyperedge_index tensor\n",
    "hyperedge_index = torch.tensor(np.array([\n",
    "    np.concatenate(hypergraph_edges),\n",
    "    np.repeat(np.arange(len(hypergraph_edges)), [len(e) for e in hypergraph_edges])\n",
    "]), dtype=torch.long)\n",
    "\n",
    "print(hyperedge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      " HMETIS 2.0pre1  Copyright 1998-2007, Regents of the University of Minnesota\n",
      "\n",
      "HyperGraph Information -----------------------------------------------------\n",
      " Name: ./data/ibm01.hgr, #Vtxs: 12752, #Hedges: 14111, #Cons: 1\n",
      "\n",
      "Options --------------------------------------------------------------------\n",
      " ptype=rb, ctype=H1, rtype=FAST, otype=cut, dbglvl=8\n",
      " kwayrefine: No, reconstruct: No, fixed: No\n",
      " Nruns: 1, NV-cycles: 1, CMaxNet: 50, RMaxNet: 50, Seed: 42\n",
      " #Parts: 2, UBfactor:   2.00\n",
      "\n",
      "Partitioning... ------------------------------------------------------------\n",
      "\n",
      "  Bisecting a hgraph of size [vertices=12752, hedges=14111, balance=0.50]\n",
      "\n",
      "    The mincut for this bisection =     204.00 (average = 204.0) (balance = [0.49]\n",
      " --------------------------------------------------------------------------\n",
      "  Summary for the 2-way partition:\n",
      "                Hyperedge Cut:        204.00\t\t(minimize)\n",
      "      Sum of External Degrees:        408.00\t\t(minimize)\n",
      "                  Scaled Cost:      5.02e-06\t\t(minimize)\n",
      "                   Absorption:      14012.06\t\t(maximize)\n",
      "\n",
      "      Partition Sizes & External Degrees:\n",
      "\t(  6192.000)[ 204.000] (  6560.000)[ 204.000] \n",
      "\n",
      "Timing Information ---------------------------------------------------------\n",
      "  Partitioning Time:\t\t   0.320sec\n",
      "           I/O Time:\t\t   0.041sec\n",
      "*******************************************************************************\n",
      "(12752, 7)\n"
     ]
    }
   ],
   "source": [
    "from utils import create_clique_expansion_graph, compute_topological_features, create_partition_id_feature\n",
    "\n",
    "adj_matrix, node_degree, pin_count = create_clique_expansion_graph(hypergraph_vertices, hypergraph_edges)\n",
    "clique_topo_features = compute_topological_features(adj_matrix, 2, True, False)\n",
    "star_topo_features = torch.tensor(U.copy(), dtype=torch.float)\n",
    "partition_feature = create_partition_id_feature(len(hypergraph_vertices), filename)\n",
    "features = np.column_stack([clique_topo_features, star_topo_features, node_degree, pin_count, partition_feature])\n",
    "D = torch.tensor(pin_count, dtype=torch.float).unsqueeze(1)\n",
    "del adj_matrix, node_degree, pin_count, clique_topo_features, star_topo_features, partition_feature\n",
    "deg_feature_norm = np.linalg.norm(features[:, 4])\n",
    "features[:, 0] = features[:, 0] / np.linalg.norm(features[:, 0]) * deg_feature_norm\n",
    "features[:, 1] = features[:, 1] / np.linalg.norm(features[:, 1]) * deg_feature_norm\n",
    "features[:, 2] = features[:, 2] / np.linalg.norm(features[:, 2]) * deg_feature_norm\n",
    "features[:, 3] = features[:, 3] / np.linalg.norm(features[:, 3]) * deg_feature_norm\n",
    "features[:, 5] = features[:, 5] / np.linalg.norm(features[:, 5]) * deg_feature_norm\n",
    "features[:, 6] = features[:, 6] / np.linalg.norm(features[:, 6]) * deg_feature_norm\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([12752, 14111])\n",
      "torch.Size([12752, 1])\n",
      "torch.Size([12752, 7])\n"
     ]
    }
   ],
   "source": [
    "from models import NewHyperData\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "data = NewHyperData(x=x, hyperedge_index=hyperedge_index)\n",
    "data = data.to(device)\n",
    "W = torch.sparse_coo_tensor(hyperedge_index, torch.ones(hyperedge_index.shape[1]), (num_nodes, num_nets)).to(device)\n",
    "D = D.to(device)\n",
    "print(device)\n",
    "print(W.shape)\n",
    "print(D.shape)\n",
    "print(data.x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 348423.96875, KL Loss: 475.24908447265625, Ncut Loss: 0.7299512624740601, Balance Loss: 348420.09375\n",
      "Epoch 2, Loss: 84215.6796875, KL Loss: 389.5168151855469, Ncut Loss: 0.7293936610221863, Balance Loss: 84211.8359375\n",
      "Epoch 3, Loss: 10444.798828125, KL Loss: 360.5218505859375, Ncut Loss: 0.7292904853820801, Balance Loss: 10440.9716796875\n",
      "Epoch 4, Loss: 8088.6748046875, KL Loss: 353.5520935058594, Ncut Loss: 0.7291035652160645, Balance Loss: 8084.8525390625\n",
      "Epoch 5, Loss: 40892.8828125, KL Loss: 350.8944396972656, Ncut Loss: 0.729270339012146, Balance Loss: 40889.0625\n",
      "Epoch 6, Loss: 64406.1171875, KL Loss: 351.213134765625, Ncut Loss: 0.7295522689819336, Balance Loss: 64402.29296875\n",
      "Epoch 7, Loss: 77677.2421875, KL Loss: 355.051513671875, Ncut Loss: 0.729552149772644, Balance Loss: 77673.4140625\n",
      "Epoch 8, Loss: 47744.625, KL Loss: 353.5704040527344, Ncut Loss: 0.7292856574058533, Balance Loss: 47740.80078125\n",
      "Epoch 9, Loss: 30413.00390625, KL Loss: 359.64453125, Ncut Loss: 0.7295994162559509, Balance Loss: 30409.17578125\n",
      "Epoch 10, Loss: 7686.26025390625, KL Loss: 364.232666015625, Ncut Loss: 0.7294898629188538, Balance Loss: 7682.4306640625\n",
      "Epoch 11, Loss: 211.8019256591797, KL Loss: 371.2021484375, Ncut Loss: 0.729576826095581, Balance Loss: 207.96844482421875\n",
      "Epoch 12, Loss: 3504.328369140625, KL Loss: 379.5175476074219, Ncut Loss: 0.7294902205467224, Balance Loss: 3500.4912109375\n",
      "Epoch 13, Loss: 9918.1943359375, KL Loss: 384.67926025390625, Ncut Loss: 0.7293818593025208, Balance Loss: 9914.35546875\n",
      "Epoch 14, Loss: 22056.712890625, KL Loss: 388.0929870605469, Ncut Loss: 0.7293986678123474, Balance Loss: 22052.87109375\n",
      "Epoch 15, Loss: 30978.8125, KL Loss: 384.6706237792969, Ncut Loss: 0.7296023368835449, Balance Loss: 30974.97265625\n",
      "Epoch 16, Loss: 27089.892578125, KL Loss: 378.92401123046875, Ncut Loss: 0.7295165061950684, Balance Loss: 27086.0546875\n",
      "Epoch 17, Loss: 17243.75390625, KL Loss: 366.9769287109375, Ncut Loss: 0.7294396162033081, Balance Loss: 17239.923828125\n",
      "Epoch 18, Loss: 7069.69287109375, KL Loss: 358.4308776855469, Ncut Loss: 0.7297232151031494, Balance Loss: 7065.865234375\n",
      "Epoch 19, Loss: 4027.23583984375, KL Loss: 349.36785888671875, Ncut Loss: 0.7296814322471619, Balance Loss: 4023.412841796875\n",
      "Epoch 20, Loss: 147.7549285888672, KL Loss: 341.99859619140625, Ncut Loss: 0.7295673489570618, Balance Loss: 143.93609619140625\n",
      "Epoch 21, Loss: 2354.657470703125, KL Loss: 333.1979675292969, Ncut Loss: 0.7294567227363586, Balance Loss: 2350.843505859375\n",
      "Epoch 22, Loss: 3762.864501953125, KL Loss: 326.3941650390625, Ncut Loss: 0.7293387651443481, Balance Loss: 3759.0546875\n",
      "Epoch 23, Loss: 9002.298828125, KL Loss: 321.88568115234375, Ncut Loss: 0.7293860912322998, Balance Loss: 8998.4912109375\n",
      "Epoch 24, Loss: 8210.6884765625, KL Loss: 318.7152099609375, Ncut Loss: 0.7295171022415161, Balance Loss: 8206.8818359375\n",
      "Epoch 25, Loss: 10439.6962890625, KL Loss: 313.5762634277344, Ncut Loss: 0.7295125126838684, Balance Loss: 10435.8916015625\n",
      "Epoch 26, Loss: 5835.38037109375, KL Loss: 311.8899841308594, Ncut Loss: 0.7292858362197876, Balance Loss: 5831.578125\n",
      "Epoch 27, Loss: 5598.32763671875, KL Loss: 308.8596496582031, Ncut Loss: 0.729362964630127, Balance Loss: 5594.5263671875\n",
      "Epoch 28, Loss: 987.7650146484375, KL Loss: 307.677001953125, Ncut Loss: 0.7294870018959045, Balance Loss: 983.9637451171875\n",
      "Epoch 29, Loss: 430.02508544921875, KL Loss: 304.90234375, Ncut Loss: 0.7293484807014465, Balance Loss: 426.22589111328125\n",
      "Epoch 30, Loss: 105.74833679199219, KL Loss: 303.57940673828125, Ncut Loss: 0.7294780015945435, Balance Loss: 101.94915771484375\n",
      "Epoch 31, Loss: 775.383544921875, KL Loss: 302.44573974609375, Ncut Loss: 0.7294577956199646, Balance Loss: 771.5850219726562\n",
      "Epoch 32, Loss: 2431.12109375, KL Loss: 301.2569885253906, Ncut Loss: 0.7294595241546631, Balance Loss: 2427.3232421875\n",
      "Epoch 33, Loss: 3977.519775390625, KL Loss: 300.3935546875, Ncut Loss: 0.7295295596122742, Balance Loss: 3973.721923828125\n",
      "Epoch 34, Loss: 4271.54052734375, KL Loss: 299.7744445800781, Ncut Loss: 0.7294713854789734, Balance Loss: 4267.7431640625\n",
      "Epoch 35, Loss: 5013.4697265625, KL Loss: 299.54034423828125, Ncut Loss: 0.7294468283653259, Balance Loss: 5009.6728515625\n",
      "Epoch 36, Loss: 1325.3995361328125, KL Loss: 299.20916748046875, Ncut Loss: 0.7294716835021973, Balance Loss: 1321.6025390625\n",
      "Epoch 37, Loss: 189.8131103515625, KL Loss: 299.82830810546875, Ncut Loss: 0.7296123504638672, Balance Loss: 186.01513671875\n",
      "Epoch 38, Loss: 74.6761703491211, KL Loss: 299.0345458984375, Ncut Loss: 0.7294520139694214, Balance Loss: 70.87939453125\n",
      "Epoch 39, Loss: 3.8423469066619873, KL Loss: 299.05084228515625, Ncut Loss: 0.7294585108757019, Balance Loss: 0.04552888870239258\n",
      "Epoch 40, Loss: 590.746826171875, KL Loss: 299.6815490722656, Ncut Loss: 0.7295164465904236, Balance Loss: 586.9494018554688\n",
      "Epoch 41, Loss: 2429.5556640625, KL Loss: 299.51318359375, Ncut Loss: 0.7294572591781616, Balance Loss: 2425.758544921875\n",
      "Epoch 42, Loss: 1048.67529296875, KL Loss: 297.6701354980469, Ncut Loss: 0.7295438647270203, Balance Loss: 1044.8787841796875\n",
      "Epoch 43, Loss: 1448.8524169921875, KL Loss: 297.4634094238281, Ncut Loss: 0.7295119762420654, Balance Loss: 1445.05615234375\n",
      "Epoch 44, Loss: 565.9019775390625, KL Loss: 295.79644775390625, Ncut Loss: 0.7294377684593201, Balance Loss: 562.1068725585938\n",
      "Epoch 45, Loss: 856.7528076171875, KL Loss: 294.6108703613281, Ncut Loss: 0.7295152544975281, Balance Loss: 852.9579467773438\n",
      "Epoch 46, Loss: 23.657054901123047, KL Loss: 294.6323547363281, Ncut Loss: 0.729501485824585, Balance Loss: 19.86223030090332\n",
      "Epoch 47, Loss: 12.508933067321777, KL Loss: 293.1268615722656, Ncut Loss: 0.7295745611190796, Balance Loss: 8.714496612548828\n",
      "Epoch 48, Loss: 296.94854736328125, KL Loss: 291.4680480957031, Ncut Loss: 0.7294275760650635, Balance Loss: 293.1556701660156\n",
      "Epoch 49, Loss: 101.42447662353516, KL Loss: 291.9989013671875, Ncut Loss: 0.7294527888298035, Balance Loss: 97.63121032714844\n",
      "Epoch 50, Loss: 358.13458251953125, KL Loss: 291.41107177734375, Ncut Loss: 0.7295123934745789, Balance Loss: 354.34130859375\n"
     ]
    }
   ],
   "source": [
    "from models import GraphPartitionModel\n",
    "\n",
    "input_dim = 7\n",
    "latent_dim = 64\n",
    "hidden_dim = 256\n",
    "num_partitions = 2\n",
    "num_epochs = 50\n",
    "model = GraphPartitionModel(input_dim, hidden_dim, latent_dim, num_partitions, True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    Y = model(data)\n",
    "    loss, kl_loss, hyperedge_cut_loss, balance_loss = model.combined_loss(Y, W, D)\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}, KL Loss: {kl_loss.item()}, Ncut Loss: {hyperedge_cut_loss.item()}, Balance Loss: {balance_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9222.130859375\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(model.hyperedge_cut_loss(Y, W).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau: 0, Best Cut: 307, Imbalance: 0.040, Cut: 307, Imbalance: 0.040\n",
      "Tau: 1, Best Cut: 286, Imbalance: 0.033, Cut: 286, Imbalance: 0.033\n",
      "Tau: 2, Best Cut: 202, Imbalance: 0.028, Cut: 202, Imbalance: 0.028\n",
      "Tau: 3, Best Cut: 201, Imbalance: 0.039, Cut: 201, Imbalance: 0.039\n",
      "Tau: 4, Best Cut: 201, Imbalance: 0.039, Cut: 252, Imbalance: 0.040\n",
      "Tau: 5, Best Cut: 201, Imbalance: 0.039, Cut: 248, Imbalance: 0.028\n",
      "Tau: 6, Best Cut: 201, Imbalance: 0.039, Cut: 242, Imbalance: 0.040\n",
      "Tau: 7, Best Cut: 201, Imbalance: 0.039, Cut: 204, Imbalance: 0.040\n",
      "Tau: 8, Best Cut: 201, Imbalance: 0.039, Cut: 211, Imbalance: 0.040\n",
      "Tau: 9, Best Cut: 201, Imbalance: 0.039, Cut: 206, Imbalance: 0.040\n",
      "Best Cut: 201, Imbalance: 0.039\n"
     ]
    }
   ],
   "source": [
    "from utils import evalPoint\n",
    "\n",
    "best_cut = float('inf')\n",
    "best_imbalance = float('inf')\n",
    "for tau in range(10):\n",
    "    partitions = model.sample(data, m=1)\n",
    "    cut, imbalance, partition_id = evalPoint(0, partitions[0], hypergraph_vertices, hypergraph_edges, 2, filename, True)\n",
    "    if cut < best_cut:\n",
    "        best_cut = cut\n",
    "        best_imbalance = imbalance\n",
    "        partition_id = partition_id / np.linalg.norm(partition_id) * np.linalg.norm(data.x[:, 4].cpu().numpy())\n",
    "        data.x[:, 6] = torch.tensor(partition_id, dtype=torch.float).to(device)\n",
    "    print(f'Tau: {tau}, Best Cut: {best_cut}, Imbalance: {best_imbalance:.3f}, Cut: {cut}, Imbalance: {imbalance:.3f}')\n",
    "print(f'Best Cut: {best_cut}, Imbalance: {best_imbalance:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evalPoint\n",
    "\n",
    "def train_rl(model, data, hypergraph_vertices, hypergraph_edges, filename, \n",
    "             num_episodes=50, learning_rate=1e-4, max_imbalance=0.04):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_cut = float('inf')\n",
    "    best_imbalance = float('inf')\n",
    "    best_partition = None\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {'episode': [], 'cut': [], 'imbalance': [], 'reward': [], 'advantage': []}\n",
    "    \n",
    "    # 记录前一次的cut\n",
    "    previous_cut = None\n",
    "    # 移动平均baseline\n",
    "    reward_baseline = 0\n",
    "    baseline_alpha = 0.9\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # 前向传播\n",
    "        logits = model(data)\n",
    "        action_probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # 温度控制\n",
    "        temperature = max(1.0 - episode / num_episodes, 0.5)\n",
    "        action_probs = action_probs ** (1 / temperature)\n",
    "        action_probs = action_probs / action_probs.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # 采样动作\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        partition = dist.sample()\n",
    "        log_probs = dist.log_prob(partition)\n",
    "        \n",
    "        # 使用传统算法优化分区\n",
    "        partition_np = partition.cpu().detach().numpy()\n",
    "        cut, imbalance, optimized_partition = evalPoint(\n",
    "            episode, partition_np, hypergraph_vertices, hypergraph_edges, 2, filename, True)\n",
    "        \n",
    "        # 基本奖励\n",
    "        base_reward = -cut\n",
    "        imbalance_penalty = max(0, imbalance - max_imbalance) * 1000\n",
    "        \n",
    "        # 相对改进奖励\n",
    "        if previous_cut is not None:\n",
    "            # 计算相对改进\n",
    "            if cut < previous_cut:\n",
    "                improvement = previous_cut - cut\n",
    "                # 非线性放大改进带来的收益\n",
    "                improvement_bonus = 500 * (1 - math.exp(-improvement * 0.1))\n",
    "                reward = base_reward + improvement_bonus - imbalance_penalty\n",
    "            else:\n",
    "                reward = base_reward - imbalance_penalty\n",
    "        else:\n",
    "            reward = base_reward - imbalance_penalty\n",
    "        \n",
    "        # 基线减法，减少方差\n",
    "        advantage = reward - reward_baseline\n",
    "        reward_baseline = baseline_alpha * reward_baseline + (1 - baseline_alpha) * reward\n",
    "        \n",
    "        # 发现新的最优解时给予额外奖励\n",
    "        if cut < best_cut:\n",
    "            best_cut = cut\n",
    "            best_imbalance = imbalance\n",
    "            best_partition = optimized_partition\n",
    "            advantage += 100  # 额外奖励\n",
    "        elif cut == best_cut and imbalance < best_imbalance:\n",
    "            best_imbalance = imbalance\n",
    "            best_partition = optimized_partition\n",
    "            advantage += 50  # 稍小的额外奖励\n",
    "        \n",
    "        # 更新前一次的cut\n",
    "        previous_cut = cut\n",
    "        \n",
    "        # 策略梯度更新\n",
    "        loss = -log_probs.mean() * advantage\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录历史\n",
    "        history['episode'].append(episode)\n",
    "        history['cut'].append(cut)\n",
    "        history['imbalance'].append(imbalance)\n",
    "        history['reward'].append(reward)\n",
    "        history['advantage'].append(advantage)\n",
    "        \n",
    "        print(f'Episode {episode+1}/{num_episodes}, Cut: {cut}, Imbalance: {imbalance:.4f}, '\n",
    "              f'Reward: {reward:.2f}, Advantage: {advantage:.2f}')\n",
    "    \n",
    "    return best_partition, best_cut, best_imbalance, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
